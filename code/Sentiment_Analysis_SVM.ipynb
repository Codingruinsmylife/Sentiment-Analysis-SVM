{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to NLP"
      ],
      "metadata": {
        "id": "FOw0lVeshOeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing (NLP)** refers to a subfield of **Artificial Intelligence** and computational linguistics that focuses on the interaction between computers and human (natural) language. NLP is conventionally done by developing algorithms, models, and tools to enable computers to understand, interpret, and generate human language. Examples of NLP are:\n",
        "\n",
        "1.   Sentiment analysis\n",
        "2.   Machine translation\n",
        "3.   Automatic summarization\n",
        "4.   Chatbot\n",
        "5.   Email filtering\n",
        "\n"
      ],
      "metadata": {
        "id": "04HfteUUhmny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "oHWs1DEfik1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several libraries and modules will be used throughout this project. Importing the necessary libraries is the initial step to begin the project.\n",
        "\n",
        "1.   **Pandas** - Pandas is a powerful data manipulation and analysis library for Python as it provides data structures and functions to work with structured data like tables.\n",
        "2.   **Numpy** - Numpy is a fundamental package for numerical computations in Python  as it provides support for arrays and matrices along with mathematical functions to operate on them efficiently.\n",
        "3.   **NLTK** - NLTK stands for Natural Language Toolkit, a library for working with human language data. It provides easy-to-use interfaces to linguistic data and models for natural language processing tasks.\n",
        "4.   **Gzip** - Gzip is a module that proJson module provides methods for working with JSON data which is a common data format for storing and exchanging structured datavides support for reading and writing GZIP-compressed files. It will be used in this project due to the input dataset is a GZIP-compressed JSON file.\n",
        "5.   **Json** - Json module provides methods for working with JSON data which is a common data format for storing and exchanging structured data.\n",
        "\n"
      ],
      "metadata": {
        "id": "qmV4uoDYinuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import gzip\n",
        "import json"
      ],
      "metadata": {
        "id": "s-cwLWH1aNnU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON GZ dataset\n",
        "def load_json_gz_dataset(filename):\n",
        "    data = []\n",
        "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Load and preprocess the JSON GZ dataset\n",
        "dataset = load_json_gz_dataset(\"Magazine_Subscriptions.json.gz\")\n",
        "\n",
        "# Convert the dataset to a Pandas DataFrame\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# Display some sample data\\n\",\n",
        "print(\"Sample Data:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "CqFEBpRBabfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "QVrxTl67jSDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocessing phase of this project involved stopwords removal. **Stopwords** are common words (e.g. \"the\", \"and\", \"in\") that are often removed from text data since they do not carry significant meaning. A set of English stopwords is generated using the NLTK stopwords dataset to filter out stopwords from the text. Another text preprocessing phase will be stemming. **Stemming** is a process of reducing the words to their root or base form (e.g. \"improving\", \"improvement\" -> \"improv\") which is helpful in text analysis by reducing inflected words to a common form."
      ],
      "metadata": {
        "id": "9erC27_ZjVf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Preprocess the text data\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):  # Exclude non-string 'text'\n",
        "        text = text.lower()\n",
        "        text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "        text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "df[\"reviewText\"] = df[\"reviewText\"].apply(preprocess_text)\n",
        "\n",
        "# Map 'overall' ratings (1-5) to 'sentiment' labels (\"Positive\", \"Neutral\", \"Negative\")\n",
        "def map_rating_to_sentiment(rating):\n",
        "    if rating <= 2:\n",
        "        return 'negative'\n",
        "    elif rating == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'positive'\n",
        "\n",
        "df[\"sentiment\"] = df[\"overall\"].apply(map_rating_to_sentiment)"
      ],
      "metadata": {
        "id": "TLWOEGM7a-vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Model Training Data"
      ],
      "metadata": {
        "id": "86fYDRClj6AE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before feeding data to train a model, **TF-IDF (Term Frequency-Inverse Data Frequency)** vectorization has to be performed on text data. TF-IDF is an essential technique for converting text documents into a numerical format that machine learning algorithms can work with."
      ],
      "metadata": {
        "id": "yjF3CYJzkAv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing or NaN values in the \"reviewText\" column\n",
        "df.dropna(subset=[\"reviewText\"], inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = df[\"reviewText\"]\n",
        "y = df[\"sentiment\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bw5uE_dRcCWy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "k5Bs_A83cZV7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "Bn73LCmVkLsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vector Machine (SVM)** will be used in this project as it is a popular and powerful machine learning algorithm that is used in various fields for classification and regression tasks. SVMs are particularly **effective when dealing with high-dimensional feature spaces** which makes them well-suited for tasks involving text data, images, and other data types with numerous features. The main objective of SVM is to **find a decision boundary that best separates data points into different classes** and another objective is to **find a hyperplane that maximizes the margen between the nearest data points of different classes**."
      ],
      "metadata": {
        "id": "8dkkZCoBkOMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Perform hyperparameter tuning\n",
        "# param_grid = {'C': [0.1, 1, 10],\n",
        "#               'kernel': ['linear']}\n",
        "\n",
        "# svm_classifier = SVC(random_state=42)\n",
        "# grid_search = GridSearchCV(svm_classifier, param_grid, cv=3, scoring='accuracy', verbose=2)\n",
        "# grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# # Get the best parameters and model\n",
        "# best_params = grid_search.best_params_\n",
        "# svm_classifier = grid_search.best_estimator_\n",
        "\n",
        "# print(\"\\nBest Hyperparameters:\")\n",
        "# print(best_params)"
      ],
      "metadata": {
        "id": "MmJir72kchNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "JdAU1N_mBT87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Model Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\", report)"
      ],
      "metadata": {
        "id": "rAhrga8gc4ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model's correctness\n",
        "def predict_sentiment(text):\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    text_tfidf = tfidf_vectorizer.transform([preprocessed_text])\n",
        "    sentiment = best_model.predict(text_tfidf)\n",
        "    return sentiment[0]\n",
        "\n",
        "sample_texts = [\n",
        "    \"This product is amazing and I love it!\",\n",
        "    \"The service was terrible and I'm highly disappointed.\",\n",
        "    \"Neutral sentiment for this one.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting the Model on Sample Texts:\")\n",
        "for text in sample_texts:\n",
        "    sentiment = predict_sentiment(text)\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Predicted Sentiment: {sentiment}\\n\")"
      ],
      "metadata": {
        "id": "hzifmVXo7kjg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}