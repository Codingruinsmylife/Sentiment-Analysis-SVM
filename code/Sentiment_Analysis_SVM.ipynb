{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOw0lVeshOeV"
   },
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04HfteUUhmny"
   },
   "source": [
    "**Natural Language Processing (NLP)** refers to a subfield of **Artificial Intelligence** and computational linguistics that focuses on the interaction between computers and human (natural) language. NLP is conventionally done by developing algorithms, models, and tools to enable computers to understand, interpret, and generate human language. Examples of NLP are:\n",
    "\n",
    "1.   Sentiment analysis\n",
    "2.   Machine translation\n",
    "3.   Automatic summarization\n",
    "4.   Chatbot\n",
    "5.   Email filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHWs1DEfik1y"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmV4uoDYinuy"
   },
   "source": [
    "Several libraries and modules will be used throughout this project. Importing the necessary libraries is the initial step to begin the project.\n",
    "\n",
    "1.   **Pandas** - Pandas is a powerful data manipulation and analysis library for Python as it provides data structures and functions to work with structured data like tables.\n",
    "2.   **Numpy** - Numpy is a fundamental package for numerical computations in Python  as it provides support for arrays and matrices along with mathematical functions to operate on them efficiently.\n",
    "3.   **NLTK** - NLTK stands for Natural Language Toolkit, a library for working with human language data. It provides easy-to-use interfaces to linguistic data and models for natural language processing tasks.\n",
    "4.   **Gzip** - Gzip is a module that proJson module provides methods for working with JSON data which is a common data format for storing and exchanging structured datavides support for reading and writing GZIP-compressed files. It will be used in this project due to the input dataset is a GZIP-compressed JSON file.\n",
    "5.   **Json** - Json module provides methods for working with JSON data which is a common data format for storing and exchanging structured data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s-cwLWH1aNnU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CqFEBpRBabfy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data:\n",
      "   overall vote  verified   reviewTime      reviewerID        asin  \\\n",
      "0      5.0    9     False   11 8, 2001   AH2IFH762VY5U  B00005N7P0   \n",
      "1      5.0    9     False  10 31, 2001   AOSFI0JEYU4XM  B00005N7P0   \n",
      "2      3.0   14     False  03 24, 2007  A3JPFWKS83R49V  B00005N7OJ   \n",
      "3      5.0   13     False  11 10, 2006  A19FKU6JZQ2ECJ  B00005N7OJ   \n",
      "4      5.0  NaN      True  07 14, 2014  A25MDGOMZ2GALN  B00005N7P0   \n",
      "\n",
      "         reviewerName                                         reviewText  \\\n",
      "0        ted sedlmayr  for computer enthusiast, MaxPC is a welcome si...   \n",
      "1     Amazon Customer  Thank god this is not a Ziff Davis publication...   \n",
      "2         Bryan Carey  Antiques Magazine is a publication made for an...   \n",
      "3  Patricia L. Porada  This beautiful magazine is in itself a work of...   \n",
      "4               Alvey                          A great read every issue.   \n",
      "\n",
      "                                           summary  unixReviewTime  \\\n",
      "0           AVID READER SINCE \"boot\"  WAS THE NAME      1005177600   \n",
      "1                               The straight scoop      1004486400   \n",
      "2  Antiques Magazine is Good, but not for Everyone      1174694400   \n",
      "3                           THE  DISCERNING READER      1163116800   \n",
      "4                                       Five Stars      1405296000   \n",
      "\n",
      "                            style image  \n",
      "0                             NaN   NaN  \n",
      "1                             NaN   NaN  \n",
      "2  {'Format:': ' Print Magazine'}   NaN  \n",
      "3  {'Format:': ' Print Magazine'}   NaN  \n",
      "4                             NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load the JSON GZ dataset\n",
    "def load_json_gz_dataset(filename):\n",
    "    data = []\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load and preprocess the JSON GZ dataset\n",
    "dataset = load_json_gz_dataset(\"Magazine_Subscriptions.json.gz\")\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Display some sample data\\n\",\n",
    "print(\"Sample Data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVrxTl67jSDr"
   },
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9erC27_ZjVf2"
   },
   "source": [
    "The preprocessing phase of this project involved stopwords removal. **Stopwords** are common words (e.g. \"the\", \"and\", \"in\") that are often removed from text data since they do not carry significant meaning. A set of English stopwords is generated using the NLTK stopwords dataset to filter out stopwords from the text. Another text preprocessing phase will be stemming. **Stemming** is a process of reducing the words to their root or base form (e.g. \"improving\", \"improvement\" -> \"improv\") which is helpful in text analysis by reducing inflected words to a common form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TLWOEGM7a-vK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#  Preprocess the text data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Exclude non-string 'text'\n",
    "        text = text.lower()\n",
    "        text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "        text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "df[\"reviewText\"] = df[\"reviewText\"].apply(preprocess_text)\n",
    "\n",
    "# Map 'overall' ratings (1-5) to 'sentiment' labels (\"Positive\", \"Neutral\", \"Negative\")\n",
    "def map_rating_to_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 'negative'\n",
    "    elif rating == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "df[\"sentiment\"] = df[\"overall\"].apply(map_rating_to_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86fYDRClj6AE"
   },
   "source": [
    "## Preparing Model Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjF3CYJzkAv6"
   },
   "source": [
    "Before feeding data to train a model, **TF-IDF (Term Frequency-Inverse Data Frequency)** vectorization has to be performed on text data. TF-IDF is an essential technique for converting text documents into a numerical format that machine learning algorithms can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bw5uE_dRcCWy"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing or NaN values in the \"reviewText\" column\n",
    "df.dropna(subset=[\"reviewText\"], inplace=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[\"reviewText\"]\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k5Bs_A83cZV7"
   },
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn73LCmVkLsp"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dkkZCoBkOMi"
   },
   "source": [
    "**Support Vector Machine (SVM)** will be used in this project as it is a popular and powerful machine learning algorithm that is used in various fields for classification and regression tasks. SVMs are particularly **effective when dealing with high-dimensional feature spaces** which makes them well-suited for tasks involving text data, images, and other data types with numerous features. The main objective of SVM is to **find a decision boundary that best separates data points into different classes** and another objective is to **find a hyperplane that maximizes the margen between the nearest data points of different classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MmJir72kchNs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END ...............................C=0.1, kernel=linear; total time= 6.1min\n",
      "[CV] END ...............................C=0.1, kernel=linear; total time= 5.8min\n",
      "[CV] END ...............................C=0.1, kernel=linear; total time= 5.7min\n",
      "[CV] END .................................C=1, kernel=linear; total time= 5.9min\n",
      "[CV] END .................................C=1, kernel=linear; total time= 5.9min\n",
      "[CV] END .................................C=1, kernel=linear; total time= 5.9min\n",
      "[CV] END ................................C=10, kernel=linear; total time=14.5min\n",
      "[CV] END ................................C=10, kernel=linear; total time=14.8min\n",
      "[CV] END ................................C=10, kernel=linear; total time=13.5min\n",
      "\n",
      "Best Hyperparameters:\n",
      "{'C': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# # Perform hyperparameter tuning\n",
    "# param_grid = {'C': [0.1, 1, 10],\n",
    "#               'kernel': ['linear']}\n",
    "\n",
    "# svm_classifier = SVC(random_state=42)\n",
    "# grid_search = GridSearchCV(svm_classifier, param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "# grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# # Get the best parameters and model\n",
    "# best_params = grid_search.best_params_\n",
    "# svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# print(\"\\nBest Hyperparameters:\")\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JdAU1N_mBT87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rAhrga8gc4ID"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "Accuracy: 0.8444122239571715\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.72      0.71      3314\n",
      "     neutral       0.45      0.08      0.14      1370\n",
      "    positive       0.88      0.95      0.92     13248\n",
      "\n",
      "    accuracy                           0.84     17932\n",
      "   macro avg       0.68      0.58      0.59     17932\n",
      "weighted avg       0.82      0.84      0.82     17932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hzifmVXo7kjg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the Model on Sample Texts:\n",
      "Text: 'This product is amazing and I love it!'\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Text: 'The service was terrible and I'm highly disappointed.'\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "Text: 'Neutral sentiment for this one.'\n",
      "Predicted Sentiment: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test model's correctness\n",
    "def predict_sentiment(text):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    text_tfidf = tfidf_vectorizer.transform([preprocessed_text])\n",
    "    sentiment = svm_classifier.predict(text_tfidf)\n",
    "    return sentiment[0]\n",
    "\n",
    "sample_texts = [\n",
    "    \"This product is amazing and I love it!\",\n",
    "    \"The service was terrible and I'm highly disappointed.\",\n",
    "    \"Neutral sentiment for this one.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting the Model on Sample Texts:\")\n",
    "for text in sample_texts:\n",
    "    sentiment = predict_sentiment(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome To Colaboratory",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
